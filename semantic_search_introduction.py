# -*- coding: utf-8 -*-
"""semantic-search-introduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nTHW-IQT8EyazAIUujUP85-s6_0agOgp
"""

!pip install pinecone

import os
from getpass import getpass

os.environ["PINECONE_API_KEY"] = os.getenv("PINECONE_API_KEY") or getpass(
    "Enter PineCone API Key: "
)

import pandas as pd
from pinecone import Pinecone, ServerlessSpec
import pinecone
from sentence_transformers import SentenceTransformer

files = pd.read_csv("sample_data/course_descriptions.csv", encoding="windows-1252")
files.head()

def create_course_description(row):
  description = f'''The course name is {row['course_name']}, the slug is {row['course_slug']}, and the technology is {row['course_technology']} and the course topic is {row['course_topic']}'''
  return description

files['course_description_new'] = files.apply(create_course_description, axis=1)

files['course_description_new']

from posix import environ
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"], environment="gcp-starter")
pc.list_indexes()

index_name = 'new-index'
dimension = 384
metric = 'cosine'

if index_name in [index_name for index in pc.list_indexes()]:
  pc.delete_index(index_name)
  print(f"Index {index_name} deleted")
else:
  print(f"Index {index_name} does not exist")

pc.create_index(
    name=index_name,
    dimension=dimension,
    metric=metric,
    spec=ServerlessSpec(
        cloud='aws',
        region='us-east-1'
    )
)

index = pc.Index(index_name)

"""#Embedding the Data"""

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

def create_embedding(row):
  combined_text = ' '.join([str(row[field]) for field in ['course_description', 'course_description_new', 'course_description_short']])
  embedding = model.encode(combined_text, show_progress_bar=False)
  return embedding

files['embedding'] = files.apply(create_embedding, axis=1)

vectors_to_upsert = [(str(row["course_name"]), row["embedding"].tolist()) for i, row in files.iterrows()]
index.upsert(vectors = vectors_to_upsert)

"""#Semantic Search"""

query = "clustering"
query_embedding = model.encode(query, show_progress_bar=False).tolist()

query_results = index.query(
    vector=query_embedding,
    top_k=5,
    include_values=True
)

query_results

for query_match in query_results["matches"]:
  print(f"Matched item ID: {query_match['id']}")
  print(f"Matched item score: {query_match['score']}")

